<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: 内核 | Keen on Art of Tech]]></title>
  <link href="http://tinyxd.me/tags/内核/atom.xml" rel="self"/>
  <link href="http://tinyxd.me/"/>
  <updated>2012-07-26T23:30:07+08:00</updated>
  <id>http://tinyxd.me/</id>
  <author>
    <name><![CDATA[Tiny]]></name>
    <email><![CDATA[admin@tinyxd.me]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[linux进程调度]]></title>
    <link href="http://tinyxd.me/blog/2012/07/26/linux-process-scheduling/"/>
    <updated>2012-07-26T23:18:00+08:00</updated>
    <id>http://tinyxd.me/blog/2012/07/26/linux-process-scheduling</id>
    <content type="html"><![CDATA[<p><strong><em>摘要</em></strong>：linux与任何分时系统一样，通过一个进程到另一个进程的快速切换，达到表面上看来的多个进程同时执行的神奇效果。</p>

<br />


<p>Linux的调度基于分时（time sharing）技术：多个进程以“时间多路复用”方式运行，因为CPU的时间被分成“片（slice）”，给每个可运行进程分配一片。</p>

<p>在linux中，进程的优先级是动态的。调度程序跟踪进程正在做什么，并周期性地调整它们的优先级。</p>

<h2>分类 </h2>

<p>传统上，把进程分类为“IO受限（IO bound）”或“CPU受限（CPU bound）”。前者频繁使用IO设备，并花费很多时间等待IO操作的完成；而后者则需要大量CPU时间的数值计算应用程序。</p>

<p>另一种分类方法把进程区分为三类：交互式进程（interactive process）、批处理进程（batch process、实时进程（real-time process）。</p>

<!--more-->


<p></p>

<h2>进程的抢占 </h2>

<p>Linux的进程是抢占式的。根据优先级和时间片是否国企决定是否可以被抢占。</p>

<p>Linux2.6内核是抢占式的，这意味着进程无论是处于内核态还是用户态，都可能被抢占。</p>

<h2>调度算法 </h2>

<p>调度程序总能成功地找到要执行的进程。事实上，总是至少有一个可运行进程，即swapper进程，它的PID等于0，而且它只有在CPU不能执行其他进程时才执行。</p>

<p>每个Linux进程总是按照下面的调度类型被调度：</p>

<p>SCHED_FIFO（先进先出的实时进程）：直到先被执行的进程变为非可执行状态，后来的进程才被调度执行。在这种策略下，先来的进程可以执行sched_yield系统调用，自愿放弃CPU，以让权给后来的进程；</p>

<p>SCHED_RR（时间片轮转的实时进程）：轮转调度。内核为实时进程分配时间片，在时间片用完时，让下一个进程使用CPU；</p>

<p>SCHED_NORMAL（普通的分时进程）。</p>

<h2>普通进程的调度 (参考自<a href="http://hi.baidu.com/_kouu/blog/item/52471ab5e90e7c788ad4b24a.html">linux进程调度浅析</a>) </h2>

<p>实时进程调度的中心思想是，让处于可执行状态的最高优先级的实时进程尽可能地占有CPU，因为它有实时需求；而普通进程则被认为是没有实时需求的进程，于是调度程序力图让各个处于可执行状态的普通进程和平共处地分享CPU，从而让用户觉得这些进程是同时运行的。</p>

<p>每个普通进程都有它自己的静态优先级，还有动态优先级。</p>

<p>与实时进程相比，普通进程的调度要复杂得多。内核需要考虑两件麻烦事：</p>

<p>一、动态调整进程的优先级</p>

<p>按进程的行为特征，可以将进程分为“交互式进程”和“批处理进程”：</p>

<p>交互式进程（如桌面程序、服务器、等）主要的任务是与外界交互。这样的进程应该具有较高的优先级，它们总是睡眠等待外界的输入。而在输入到来，内核将其唤醒时，它们又应该很快被调度执行，以做出响应。比如一个桌面程序，如果鼠标点击后半秒种还没反应，用户就会感觉系统“卡”了；</p>

<p>批处理进程（如编译程序）主要的任务是做持续的运算，因而它们会持续处于可执行状态。这样的进程一般不需要高优先级，比如编译程序多运行了几秒种，用户多半不会太在意；</p>

<p>如果用户能够明确知道进程应该有怎样的优先级，可以通过nice、setpriority系统调用来对优先级进行设置。（如果要提高进程的优先级，要求用户进程具有CAP_SYS_NICE能力。）</p>

<p>然而应用程序未必就像桌面程序、编译程序这样典型。程序的行为可能五花八门，可能一会儿像交互式进程，一会儿又像批处理进程。以致于用户难以给它设置一个合适的优先级。</p>

<p>再者，即使用户明确知道一个进程是交互式还是批处理，也多半碍于权限或因为偷懒而不去设置进程的优先级。（你又是否为某个程序设置过优先级呢？）</p>

<p>于是，最终，区分交互式进程和批处理进程的重任就落到了内核的调度程序上。</p>

<p>调度程序关注进程近一段时间内的表现（主要是检查其睡眠时间和运行时间），根据一些经验性的公式，判断它现在是交互式的还是批处理的？程度如何？最后决定给它的优先级做一定的调整。</p>

<p>进程的优先级被动态调整后，就出现了两个优先级：</p>

<p>1、用户程序设置的优先级（如果未设置，则使用默认值），称为静态优先级。这是进程优先级的基准，在进程执行的过程中往往是不改变的；</p>

<p>2、优先级动态调整后，实际生效的优先级。这个值是可能时时刻刻都在变化的；</p>

<p>二、调度的公平性</p>

<p>在支持多进程的系统中，理想情况下，各个进程应该是根据其优先级公平地占有CPU。而不会出现“谁运气好谁占得多”这样的不可控的情况。</p>

<p>linux实现公平调度基本上是两种思路：</p>

<p>1、给处于可执行状态的进程分配时间片（按照优先级），用完时间片的进程被放到“过期队列”中。等可执行状态的进程都过期了，再重新分配时间片；</p>

<p>2、动态调整进程的优先级。随着进程在CPU上运行，其优先级被不断调低，以便其他优先级较低的进程得到运行机会；</p>

<p>后一种方式有更小的调度粒度，并且将“公平性”与“动态调整优先级”两件事情合而为一，大大简化了内核调度程序的代码。因此，这种方式也成为内核调度程序的新宠。</p>

<p>强调一下，以上两点都是仅针对普通进程的。而对于实时进程，内核既不能自作多情地去动态调整优先级，也没有什么公平性可言。</p>

<p>普通进程具体的调度算法非常复杂，并且随linux内核版本的演变也在不断更替（不仅仅是简单的调整），所以本文就不继续深入了。有兴趣的朋友可以参考下面的链接：</p>

<p><a href="http://www.ibm.com/developerworks/cn/linux/l-cn-scheduler/">《Linux 调度器发展简述》</a></p>

<p><a href="http://blog.chinaunix.net/u1/42957/showart.php?id=337597">《鼠眼看Linux调度器》</a></p>

<p><a href="http://blog.chinaunix.net/u1/42957/showart.php?id=337604">《鼠眼再看Linux调度器［1］》</a></p>

<p><a href="http://blog.chinaunix.net/u1/42957/showart.php?id=337607">《鼠眼再看Linux调度器［2］》</a></p>

<h2>实时进程的调度 </h2>

<p>每个实时进程都与一个实时优先级相关，实时优先级是一个范围从1（最高优先级）~99（最低优先级）的值。调度程序总是让优先级高的进程运行，换句话说，实时进程运行的过程中，禁止低优先级的进程的执行。与普通进程相反，实时进程总是被当成活动进程。用户可以通过系统调用sched_setparam()和sched_setscheduler()改变进程的实时优先级。</p>

<p>只有在下述时间之一发生时，实时进程才会被另一个进程取代：</p>

<pre><code>进程被另外一个具有更高实时优先级的实时进程抢占 

进程执行了阻塞操作并进入睡眠（处于TASK_INTERRUPTIBLE或TASK_UNINTERRUPTIBLE状态）。 

进程停止（处于TASK_STOPPED或TASK_TRACED状态）或被杀死（处于EXIT_ZOMBIE或EXIT_DEAD状态）。 

进程通过调用系统调用sched_yield()自愿放弃CPU。 

进程是基于时间片轮转的实时进程（SCHED_RR），而且用完了它的时间片。 
</code></pre>

<p>数据结构runqueue是Linux2.6调度程序最重要的数据结构。系统中的每个CPU都有它自己的运行队列，所有的runqueue结构存放在runqueues每CPU变量中。</p>

<h2>调度程序所使用的函数 </h2>

<p>scheduler_tick()：维持当前最新的time_slice计数器</p>

<p>try_to_wake_up()：唤醒睡眠进程</p>

<p>recalc_task_prio()：更新进程的动态优先级</p>

<p>schedule()：选择要被执行的新进程</p>

<p>load_balance()：维持多处理器系统中运行队列的平衡</p>

<h2>多处理器系统中运行队列的平衡 </h2>

<p>Linux一直坚持采用对称多处理模式，这意味着，与其他CPU相比，内核不对一个CPU有任何偏向，但是，多处理器机器具有很多不同的风格，而且调度程序的实现随硬件特征的不同而有所不同，我们将特别关注下面三种不同类型的多处理器机器：</p>

<p>（1）标准的多处理器体系结构</p>

<p>直到最近，这是多处理器机器最普通的体系结构。这些机器所共有的RAM芯片集被所有CPU共享。</p>

<p>（2）超线程</p>

<p>超线程芯片是一个立刻执行几个执行线程的微处理器；它包括几个内部寄存器的拷贝，并快速在它们之间切换。这种由Intel发明的技术，使得当前线程在访问内存的间隙，处理器可以使用它的机器周期去执行另外一个线程。一个超线程的物理CPU可以被Linux看作几个不同的逻辑CPU。</p>

<p>（3）NUMA</p>

<p>把CPU和RAM以本地“结点”为单位分组，（通常一个结点包括一个CPU和几个RAM芯片）。内存仲裁器（一个使系统中的CPU以串型方式访问RAM的专用电路）是典型的多处理器系统的瓶颈。在NUMA体系结构中，当CPU访问与它同在一个结点中的“本地”RAM芯片时，几乎没有竞争，因此访问通常是非常快的。另一方面，访问它所属结点外的“远程”RAM芯片就非常慢。</p>

<h2>与调度相关的系统调用 </h2>

<p>nice()、getpriority()和setpriority()系统调用、sched_getaffinity()和sched_setaffinity()调用</p>

<h2>与实时进程相关的系统调用 </h2>

<p>sched_getscheduler()和sched_setscheduler()系统调用、sched_getparam()和sched_setparam()系统调用、sched_yield()系统调用、sched_get_priority_min()和 sched_get_priority_max()系统调用、sched_rr_get_interval()系统调用</p>

<br />


<p>本文章参考自《深入理解linux内核》。 <br/>
本站文章如果没有特别说明，均为<strong>原创</strong>，转载请以<strong>链接</strong>方式注明本文地址：<a href="http://tinyxd.me/blog/2012/07/26/linux-process-scheduling/">http://tinyxd.me/blog/2012/07/26/linux-process-scheduling/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[linux系统定时测量]]></title>
    <link href="http://tinyxd.me/blog/2012/07/26/linux-timing-measurements/"/>
    <updated>2012-07-26T12:48:00+08:00</updated>
    <id>http://tinyxd.me/blog/2012/07/26/linux-timing-measurements</id>
    <content type="html"><![CDATA[<p>Linux内核必须完成的两种主要定时测量：</p>

<p>保存当前的时间和日期，以便通过time()、ftime()、gettimeofday()系统调用把它们返回给用户程序，也可以由内核本身把当前时间作为文件和网络包的时间戳。</p>

<p>维持定时器，这种机制能够告诉内核或用户程序某一时间间隔已经过去了。</p>

<p>一般会遇到的几种时钟和定时器电路：实时时钟（Real Time Clock RTC）、时间戳计数器（Time Stamp Counter TSC）、可编程间隔定时器（Programmable Interval Timer PIT）、CPU本地定时器（APIC中）、高精度事件定时器（HPET）、ACPI电源管理定时器。</p>

<p>Linux的计时体系结构是一组与时间流相关的内核数据结构和函数。实际上，基于80x86多处理器机器所具有的计时体系结构与单处理器机器所具有的稍有不同：</p>

<p>在三处理器系统上，所有的计时活动都是由全局定时器（可以是可编程间隔定时器也可以是高精度事件定时器）产生的中断触发的。</p>

<!--more-->


<p>在多处理器系统上，所有普通的活动（像软定时器处理）都是由全局定时器产生的中断触发的，而具体的CPU的活动（像监控当前运行进程的执行时间）是由本地APIC定时器产生的中断触发的。</p>

<p>jiffies</p>

<p>jiffies是一个计数器，用来记录自系统启动依赖产生的节拍总数。启动时，内核将该变量初始化为0，此后，每次时钟中断处理程序都会增加该变量的值。因为一秒内时钟中断的次数等于Hz，所以jiffies一秒内增加的值也就为Hz。系统运行时间以秒为单位计算，就等于jiffies/Hz。</p>

<p>xtime</p>

<p>xtime变量存放当前时间和日期；它是一个timespec类型的数据结构，该结构有两个字段：</p>

<p>1.tv_sec 存放自1970年1月1日（UTC）午夜以来经过的秒数。</p>

<p>2.tv_nsec存放自上一秒开始经过的纳秒数（它的值域范围在0-999999999之间）</p>

<p>在单处理器系统上，所有与定时有关的活动都是由IRQ线0上的可编程间隔定时器产生的中断触发的。</p>

<p>多处理器系统可以依赖两种不同的时钟中断源：可编程间隔定时器或高精度事件定时器产生的中断，以及CPU本地定时器产生的中断(监管内核代码并检测当前进程在特定CPU上已经运行了多长时间)。</p>

<p>内核在于定时相关的其他任务中必须周期性地收集若干数据用于：</p>

<pre><code>检查运行进程的CPU资源限制  

更新与本地CPU工作负载有关的统计数  

计算平均系统负载  

监管内核代码  
</code></pre>

<p>软定时器和延迟函数</p>

<p>定时器是一种软件功能，即允许在将来的某个时期，函数在给定的时间间隔用完时被调用。超时（time-out）表示与定时器相关的时间间隔已经用完的那个时刻。</p>

<p>Linux考虑两种类型的定时器，即动态定时器（dynamic timer）和间隔定时器（interval timer）。第一种类型由内核使用，而间隔定时器可以由进程在用户态创建。</p>

<p>延迟函数</p>

<p>当内核需要等待一个较短的时间间隔（比方说，不超过几毫秒）时，就不需要使用软定时器。</p>

<p>开发驱动，需要较短的时间间隔，在以上情况下，内核使用udelay()和ndelay()函数：前者接收一个微妙级的时间间隔作为它的参数，并在指定的延迟结束后返回；后者与前者类似，但是指定的延迟参数是纳秒级的。</p>

<p>本文章参考自《深入理解linux内核》。 <br/>
本站文章如果没有特别说明，均为<strong>原创</strong>，转载请以<strong>链接</strong>方式注明本文地址：<a href="http://tinyxd.me/blog/2012/07/26/linux-timing-measurements/">http://tinyxd.me/blog/2012/07/26/linux-timing-measurements/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[linux内核同步]]></title>
    <link href="http://tinyxd.me/blog/2012/07/18/linux-kernel-synchronization/"/>
    <updated>2012-07-18T15:57:00+08:00</updated>
    <id>http://tinyxd.me/blog/2012/07/18/linux-kernel-synchronization</id>
    <content type="html"><![CDATA[<h2>内核抢占</h2>

<p>抢占内核的主要特点是：一个在内核态运行的进程，可能在执行内核函数期间被另外一个进程取代。 <br/>
只有当内核正在执行异常处理程序（尤其是系统调用），而且内核抢占没有被显式地禁用时，才可能抢占内核。 <br/>
内核使用的各种同步技术：每CPU变量、原子操作、内存屏障、自旋锁、信号量、顺序锁、本地中断的禁止、本地软中断的禁止、读-拷贝-更新（RCU）。</p>

<h2>每CPU变量</h2>

<p>主要是数据结构的数组，系统的每个CPU对应数组的一个元素。 <br/>
一个CPU不应该访问与其他CPU对应的数组元素。另外，它可以随意读或修改它自己的元素而不用担心出现竞争条件，因为它是唯一有资格这么做的CPU。在单处理器和多处理器系统中，内核抢占都可能使每CPU变量产生竞争条件。总的原则是内核控制路径应该在禁用抢占的情况下访问每CPU变量。</p>

<!--more-->


<h2>原子操作</h2>

<p>若干汇编语言指令具有“读-修改-写”类型----也就是说，它们访问存储器单元两次，第一次读原值，第二次写新值。 <br/>
Linux内核提供了一个专门的atomic_t类型（一个原子计数器）和一些专门的函数和宏。在多处理器系统中，每条这样的指令都有一个lock字节（“锁定内存总线，直到这条指令执行完成”）的前缀。</p>

<h2>优化和内存屏障</h2>

<p>当使用优化的编译器时，编译器为了优化可能会重新安排汇编语言指令以便寄存器以最优的方式使用。 <br/>
内存屏障（memory barrier）原语确保，在原语之后的操作开始执行之前，原语之前的操作已经完成。</p>

<h2>自旋锁</h2>

<p>自旋锁（spin lock）是用来在多处理器环境中工作的一种特殊的锁。如果内核控制路径发现自旋锁“开着”，就获取锁并继续自己的执行。相反，如果内核控制路径发现锁由运行在另一个CPU上的内核控制路径“锁着”，就在周围“旋转”。反复执行一条紧凑的循环指令，直到锁被释放。   <br/>
一般来说，由自旋锁所保护的每个临界区都是禁止内核抢占的。在单处理器系统上，这种锁本身并不起锁的作用，自旋锁原语仅仅是禁止或启用内核抢占。  <br/>
在linux中，每个自旋锁都用spinlock_t结构表示，其中包含两个字段：</p>

<p>1.slock----表示自旋锁的状态。（1--未加锁  负数和0--加锁） <br/>
2.break_lock----表示进程正在忙等自旋锁（只在内核支持SMP和内核抢占的情况下使用该标志）</p>

<p>读写自旋锁的引入是为了增加内核的并发能力。只要没有内核控制路径堆数据结构进行修改，读写自旋锁就允许多个内核控制路径读同一数据结构。允许对数据结构并发度可以提高系统性能。 <br/>
顺序锁：与读写自旋锁非常相似，只是它为写者赋予了较高的优先级；事实上，即使在读者正在读的时候也允许写者继续运行。这种策略的好处是写者永远不会等待（除非另外一个写者正在写），缺点是有些时候读者不得不反复多次读相同的数据直到获得有效的副本。 <br/>
当读者进入临界区时，不必禁用内核抢占；另一方面，由于写者获取自旋锁，所以它进入临界区时自动禁用内核抢占。 <br/>
一般来说，在满足以下条件时才能使用顺序锁：</p>

<p>1.被保护的数据结构不包括被写者修改和被读者间接引用的指针（否则，写者可能在读者的眼鼻下就修改指针）。 <br/>
2.读者的临界区代码没有副作用（否则，多个读者的操作会与单独的读操作有不同的结果）。</p>

<h2>读-拷贝-更新（RCU）</h2>

<p>读-拷贝-更新（RCU）是为了保护在多数情况下被多个CPU读的数据结构而设计的另一种同步技术。RCU允许多个读者和写者并发执行，RCU不使用锁，就是说它不使用被所有CPU共享的锁或计数器。  <br/>
RCU同步的关键思想是：</p>

<p>1.RCU只保护被动态分配并通过指针引用的数据结构。 <br/>
2.在被RCU保护的临界区中，任何内核控制路径都不能睡眠。</p>

<p>使用RCU技术的真正困难在于：写者修改指针时不能立即释放数据结构的旧副本。实际上，写着开始修改时，正在访问数据结构的读者可能还在读旧副本。只有在CPU上的所有（潜在的）读者都执行完宏rcu_read_unlock()之后，才可以释放旧副本。 <br/>
RCU是Linux2.6中新加的功能，用在网络层和虚拟文件系统中。</p>

<h2>信号量</h2>

<p>Linux提供两种信号量：</p>

<p>1.内核信号量，由内核控制路径使用 <br/>
2.Systerm V IPC 信号量，由用户态进程使用</p>

<p>内核信号量：类似于自旋锁，因为当锁关闭着时，它不允许内核控制路径继续进行。只有可以睡眠的函数才能获取内核信号量；中断处理程序和可延迟函数都不能使用内核控制量。 <br/>
内核信号量是struct semaphore类型的对象。 <br/>
TASK_INTERRUPTIBLE是可以被信号和wake_up()唤醒的，当信号到来时，进程会被设置为可运行。 <br/>
而TASK_UNINTERRUPTIBLE只能被wake_up()唤醒。 <br/>
读/写信号量：类似于前面的“读写自旋锁”，有一点不同的是，在信号量再次变为打开之前，等待进程挂起而不是自旋。内核以严格的FIFO顺序处理等待读写信号量的所有进程。 <br/>
每个读写信号量都是有rw_semaphore结构描述的。 <br/>
补充原语（completion）：其和信号量之间的真正区别在于如何使用等待队列中包含的自旋锁。在补充原语中，自旋锁用来确保complete()和wait_for_completion()不会并发执行。在信号量中，自旋锁用于避免并发执行的down()函数弄乱信号量的数据结构。</p>

<br />


<p>本文章参考自《深入理解linux内核》。 <br/>
本站文章如果没有特别说明，均为<strong>原创</strong>，转载请以<strong>链接</strong>方式注明本文地址：<a href="http://tinyxd.me/blog/2012/07/18/linux-kernel-synchronization/">http://tinyxd.me/blog/2012/07/18/linux-kernel-synchronization/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[linux工作队列]]></title>
    <link href="http://tinyxd.me/blog/2012/07/11/linux-workqueue/"/>
    <updated>2012-07-11T23:08:00+08:00</updated>
    <id>http://tinyxd.me/blog/2012/07/11/linux-workqueue</id>
    <content type="html"><![CDATA[<h2>工作队列(workqueue)</h2>

<p>Linux中的Workqueue机制就是为了简化内核线程的创建。通过调用workqueue的接口就能创建内核线程。并且可以根据当前系统CPU的个数创建线程的数量，使得线程处理的事务能够并行化。</p>

<p>workqueue是内核中实现简单而有效的机制，他显然简化了内核daemon的创建，方便了用户的编程，</p>

<p>尽管可延迟函数和工作队列非常相似，但是它们的区别还是很大的。主要区别在于：可延迟函数运行在中断上下文中，而工作队列中的函数运行在进程上下文中。执行可阻塞函数（例如：需要访问磁盘数据块的函数）的唯一方式是在进程上下文中运行。因为在中断上下文中不可能发生进程切换。可延迟函数和工作队列中的函数都不能访问进程的用户态地址空间。事实上，可延迟函数被执行时不可能有任何正在运行的进程。另一方面，工作队列中的函数是由内核线程来执行的，因此，根本不存在它要访问的用户态地址空间。</p>

<h2>工作队列的数据结构</h2>

<p style="text-indent:2em">与工作队列相关的主要数据结构有两个：cpu_workqueue_struct和work_struct。名为workqueue_struct的描述符，包括一个有NR_CPUS个元素的数组，NR_CPUS是系统中CPU的最大数量（双核是有两个工作队列）。每个元素都是cpu_workqueue_struct类型的描述符。 

</p>


<p></p>

<!--more-->


<p>work_struct结构是对任务的抽象。在该结构中需要维护具体的任务方法，需要处理的数据，以及任务处理的时间。该结构定义如下：</p>

<p>``` c
struct work_struct {</p>

<pre><code>unsigned long pending;                /*如果函数已经在工作队列链表中，该字段值设为1，否则设为0*/ 
    struct list_head entry;                  /* 将任务挂载到queue的挂载点 */ 
    void (*func)(void *);                   /* 任务方法 */ 
    void *data;                                  /* 任务处理的数据*/ 
   void *wq_data;                           /* work的属主（通常是指向cpu_workqueue_struct描述符的父结点的指针） */ 
   strut timer_list timer;                   /* 任务延时处理定时器 */ 
</code></pre>

<p>};
```</p>

<h2>工作队列函数</h2>

<p>当用户调用workqueue的初始化接口create_workqueue或者create_singlethread_workqueue对workqueue队列进行初始化时，内核就开始为用户分配一个workqueue对象，并且将其链到一个全局的workqueue队列中。然后Linux根据当前CPU的情况，为workqueue对象分配与CPU个数相同的cpu_workqueue_struct对象，每个cpu_workqueue_struct对象都会存在一条任务队列。紧接着，Linux为每个cpu_workqueue_struct对象分配一个内核thread，即内核daemon去处理每个队列中的任务。至此，用户调用初始化接口将workqueue初始化完毕，返回workqueue的指针。</p>

<p>在初始化workqueue过程中，内核需要初始化内核线程，注册的内核线程工作比较简单，就是不断的扫描对应cpu_workqueue_struct中的任务队列，从中获取一个有效任务，然后执行该任务。所以如果任务队列为空，那么内核daemon就在cpu_workqueue_struct中的等待队列上睡眠，直到有人唤醒daemon去处理任务队列.</p>

<p>Workqueue初始化完毕之后，将任务运行的上下文环境构建起来了，但是具体还没有可执行的任务，所以，需要定义具体的work_struct对象。然后将work_struct加入到任务队列中，Linux会唤醒daemon去处理任务。</p>

<p>queue_work()（封装在work_struct描述符中）把函数插入工作队列，它接收wq和work两个指针。wq指向workqueue_struct描述符，work指向work_struct描述符。queue_work（）主要执行下面的步骤：</p>

<p>1.检查要插入的函数是否已经在工作队列中（work->pending字段等于1），如果是就结束。</p>

<p>2.把work_struct描述符加到工作队列链表中，然后把work->pending置为1。</p>

<p>3.如果工作者线程在本地CPU的cpu_workqueue_struct描述符的more_work等待队列上睡眠，该函数唤醒这个线程。</p>

<h2>预定义工作队列</h2>

<p>在绝大多数情况下，为了运行一个函数而创建整个工作者线程开销太大了。因此，内核引入叫做events的预定义工作队列，所有的内核开发者都可以随便使用它。预定义工作队列只是一个包含不同内核层函数和IO驱动程序的标准工作队列，它的workququq_struct描述符存放在keventd_wq数组中。</p>

<p>工作队列编程接口：</p>

<table border="1"> 
<tr> 
    <th>序号</th> 
    <th>接口函数</th>  
    <th>函数说明</th> 
</tr>  
<tr>  
    <td>1</td>  
    <td>create_workqueue</td> 
    <td>用于创建一个workqueue队列，为系统中的每个CPU都创建一个内核线程。输入参数：@name：workqueue的名称</td> 
</tr>  
<tr>  
    <td>2</td>  
    <td>create_singlethread_workqueue</td>  
    <td>用于创建一个workqueue队列，为系统中的每个CPU都创建一个内核线程。输入参数：@name：workqueue的名称</td> 
</tr> 
<tr> 
    <td>3</td> 
    <td>destroy_workqueue</td> 
    <td>释放workqueue队列。输入参数：@ workqueue_struct：需要释放的workqueue队列指针</td> 
</tr>  
<tr> 
    <td>4</td> 
    <td>schedule_work</td> 
    <td>调度执行一个具体的任务，执行的任务将会被挂入Linux系统提供的workqueue——keventd_wq输入参数：@ work_struct：具体任务对象指针 
</td> 
</tr>  
<tr> 
    <td>5</td> 
    <td>schedule_delayed_work</td> 
    <td>延迟一定时间去执行一个具体的任务，功能与schedule_work类似，多了一个延迟时间，输入参数：@work_struct：具体任务对象指针@delay：延迟时间</td> 
</tr>  
<tr> 
    <td>6</td> 
    <td>queue_work</td> 
    <td>调度执行一个指定workqueue中的任务。输入参数：@ workqueue_struct：指定的workqueue指针@work_struct：具体任务对象指针</td> 
</tr>  
<tr> 
    <td>7</td> 
    <td>queue_delayed_work</td> 
    <td>延迟调度执行一个指定workqueue中的任务，功能与queue_work类似，输入参数多了一个delay</td> 
</tr>  
</table>


<p></p>

<p>预定义工作队列支持函数:</p>

<table > 
<tr> 
    <td>预定义工作队列函数</td> 
    <td>等价的标准工作队列函数</td> 
</tr> 
<tr> 
    <td>schedule_work(w)</td>     
    <td>queue_work( keventd_wq,w)</td> 
</tr> 
<tr> 
    <td>schedule_delayed_work(w,d)</td>     
    <td>queue_delayed_work(keventd_wq,w,d)在任何CPU上</td> 
</tr> 
<tr> 
    <td>schedule_delayed_work_on(cpu,w,d)</td>     
    <td>queue_delayed_work(keventd_wq,w,d)（在某个指定的CPU上）</td> 
</tr> 
<tr> 
    <td>flush_scheduled_work</td>     
    <td>flush_workqueue(keventd_wq)</td> 
</tr> 
</table>


<p></p>

<p>除了一般的events队列，在linux2.6中你还会发现一些专用的工作队列。其中最重要的是快设备层使用的kblockd工作队列。</p>

<p>本文章整理自网络和《深入理解linux内核》。  <br/>
本站文章如果没有特别说明，均为<strong>原创</strong>，转载请以<strong>链接</strong>方式注明本文地址：<a href="http://tinyxd.me/blog/2012/07/11/linux-workqueue/">http://tinyxd.me/blog/2012/07/11/linux-workqueue/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[软中断和tasklet]]></title>
    <link href="http://tinyxd.me/blog/2012/07/11/soft-interrupt-and-tasklet/"/>
    <updated>2012-07-11T22:57:00+08:00</updated>
    <id>http://tinyxd.me/blog/2012/07/11/soft-interrupt-and-tasklet</id>
    <content type="html"><![CDATA[<h2>软中断：</h2>

<p>Linux中的软中断机制用于系统中对时间要求最严格以及最重要的中断下半部进行使用。在系统设计过程中，大家都清楚中断上下文不能处理太多的事情，需要快速的返回，否则很容易导致中断事件的丢失，所以这就产生了一个问题：中断发生之后的事务处理由谁来完成？在前后台程序中，由于只有中断上下文和一个任务上下文，所以中断上下文触发事件，设置标记位，任务上下文循环扫描标记位，执行相应的动作，也就是中断发生之后的事情由任务来完成了，只不过任务上下文采用扫描的方式，实时性不能得到保证。在Linux系统和Windows系统中，这个不断循环的任务就是本文所要讲述的软中断daemon。在Windows中处理耗时的中断事务称之为中断延迟处理，在Linux中称之为中断下半部，显然中断上半部处理清中断之类十分清闲的动作，然后在退出中断服务程序时触发中断下半部，完成具体的功能。</p>

<p>在Linux中，中断下半部的实现基于软中断机制。所以理清楚软中断机制的原理，那么中断下半部的实现也就非常简单了。通过上述的描述，大家也应该清楚为什么要定义软中断机制了，一句话就是为了要处理对时间要求苛刻的任务，恰好中断下半部就有这样的需求，所以其实现采用了软中断机制。</p>

<p>linux2.6中使用的软中断：</p>

<p>``` c interrupt.h
enum
{</p>

<pre><code>HI_SOFTIRQ=0, /*用于高优先级的tasklet(下标(优先级0))*/
TIMER_SOFTIRQ, /*用于定时器的下半部(下标(优先级1))*/
NET_TX_SOFTIRQ,/*用于网络层发包(下标(优先级2))*/
NET_RX_SOFTIRQ, /*用于网络层收报(下标(优先级3))*/
SCSI_SOFTIRQ, /*用于scsi设备(下标(优先级4))*/
TASKLET_SOFTIRQ /*用于低优先级的tasklet(下标(优先级5))*/
</code></pre>

<p>};
```</p>

<!--more-->


<p>一个软中断的下标决定了它的优先级；低下标意味着高优先级，因为软中断函数将从下标0开始执行。</p>

<h2>tasklet:</h2>

<p>Tasklet为一个软中断，考虑到优先级问题，分别占用了向量表中的0号和5号软中断。</p>

<p>tasklet是IO驱动程序中实现可延迟函数的首选方法。其建立在两个叫HI_SOFTIRQ和TASKLET_SOFTIRQ的软中断之上。</p>

<h2>软中断和tasklet</h2>

<p>软中断和tasklet有密切的关系，tasklet是在软中断之上实现。事实上，出现在内核代码中的术语“软中断（softirq）” 常常表示可延迟函数的所有种类。另外一种被广泛使用的术语是“中断上下文”：表示内核当前正在执行一个中断处理程序或一个可延迟的函数。</p>

<p>软中断的分配是静态的（即在编译时定义），而tasklet的分配和初始化可以在运行时进行（例如：安装一个内核模块时）。软中断（即便是同一种类型的软中断）可以并发地运行在多个CPU上。因此，软中断是可重入函数而且必须明确地使用自旋锁保护其数据结构。tasklet不必担心这些问题，因为内核对tasklet的执行进行了更加严格的控制。相同类型的tasklet总是被串行地执行，换句话说就是：不能在两个CPU上同时运行相同类型的tasklet。但是，类型不同的tasklet可以在几个CPU上并发执行。tasklet的串行化使tasklet函数不必是可重入的，因此简化了设备驱动程序开发者的工作。</p>

<p>一般而言，可延迟函数上可以执行四种函数：初始化、激活、屏蔽和执行。</p>

<h2>软中断的主要数据结构</h2>

<p>软中断的主要数据结构是softirq_vec数组，该数组包含类型为softirq_action的32个元素。一个软中断的优先级是相应的softirq_action元素在数组内的下标，只有前六个被有效使用。</p>

<p>/*表示softirq最多可以有32种类型，实际上linux只使用了六种，见文件interrupt.h*/</p>

<p>static struct softirq_action softirq_vec[32] __cacheline_aligned_in_smp;</p>

<p>softirq_action数据结构包括两个字段：指向软中断函数的一个action指针和指向软中断函数需要的通用数据结构的data指针。</p>

<p>还有一个关键的字段是32位的preempt_cout字段，用它来跟踪内核抢占和内核控制路径的嵌套，该字段存放在每个进程描述符的thread_info字段中。</p>

<p>对于softirq，linux kernel中是在中断处理程序执行的，具体的路径为：</p>

<pre><code>do_IRQ() --&gt; irq_exit() --&gt; invoke_softirq() --&gt; do_softirq() --&gt; __do_softirq() 
</code></pre>

<p>在__do_softirq()中有这么一段代码：  <br/>
``` c</p>

<pre><code>    do { 

            if (pending &amp; 1) { 

                    h-&gt;action(h); 

                    rcu_bh_qsctr_inc(cpu); 

            } 

            h++; 

            pending &gt;&gt;= 1; 

    } while (pending); 
</code></pre>

<p>```</p>

<p>你看，这里就是对softirq进行处理了，因为pengding是一个__u32的类型，所以每一位都对应了一种softirq，正好是32种（linux kernel中实际上只使用了前6种 ）. h->action(h),就是运行softirq的处理函数。</p>

<p>对于tasklet，前面已经说了，是一种特殊的softirq，具体就是第0和第5种softirq，所以说tasklet是基于softirq来实现的。</p>

<p>tasklet既然对应第0和第5种softirq，那么就应该有对应的处理函数，以便h->action()会运行tasklet的处理函数。</p>

<p>我们看代码：
``` c softirq.c</p>

<p>void __init softirq_init(void)</p>

<p>{</p>

<pre><code>    open_softirq(TASKLET_SOFTIRQ, tasklet_action, NULL); 

    open_softirq(HI_SOFTIRQ, tasklet_hi_action, NULL); 
</code></pre>

<p>}
```</p>

<p>这里注册了两种tasklet所在的softirq的处理函数，分别对应高优先级的tasklet和低优先级的tasklet。</p>

<p>我们看低优先级的吧（高优先级的也一样）。
``` c tasklet_action
static void tasklet_action(struct softirq_action *a)</p>

<p>{</p>

<pre><code>    struct tasklet_struct *list; 

    local_irq_disable(); 

    list = __get_cpu_var(tasklet_vec).list; 

    __get_cpu_var(tasklet_vec).list = NULL; 

    local_irq_enable(); 

    while (list) { 

            struct tasklet_struct *t = list; 

            list = list-&gt;next; 

            if (tasklet_trylock(t)) { 

                    if (!atomic_read(&amp;t-&gt;count)) { 

                            if (!test_and_clear_bit(TASKLET_STATE_SCHED, &amp;t-&gt;state)) 

                                    BUG(); 

                            t-&gt;func(t-&gt;data); 

                            tasklet_unlock(t); 

                            continue; 

                    } 

                    tasklet_unlock(t); 

            } 

            local_irq_disable(); 

            t-&gt;next = __get_cpu_var(tasklet_vec).list; 

            __get_cpu_var(tasklet_vec).list = t; 

            __raise_softirq_irqoff(TASKLET_SOFTIRQ);   

            local_irq_enable(); 

    } 
</code></pre>

<p>}
<code>
你看，在运行softirq的处理时（__do_softirq），对于
</code> c</p>

<pre><code>    do { 

            if (pending &amp; 1) { 

                    h-&gt;action(h); 

                    rcu_bh_qsctr_inc(cpu); 

            } 

            h++; 

            pending &gt;&gt;= 1; 

    } while (pending); 
</code></pre>

<p>```
如果tasklet有任务需要处理，会运行到h->action()，这个函数指针就会指向tasklet_action()，然后在tasklet_action()里再去执行tasklet对应的各个任务，这些任务都是挂在一个全局链表里面的，具体的代码这里就不分析了。</p>

<p>另外， softirq在smp中是可能被同时运行的，所以softirq的处理函数必须被编写成可重入的函数。</p>

<p>但tasklet是不会在多个cpu之中同时运行的，所以tasklet的处理函数可以编写成不可重入的函数，这样就减轻了编程人员的负担。</p>

<h2>ksoftirqd内核线程</h2>

<p>在最近的内核版本中，每个CPU都有自己的ksoftirqd/n内核线程（这里，n为CPU的逻辑号）。每个ksoftirqd/n内核线程都运行ksoftirqd()函数。在预期的时间内处理挂起的软中断。</p>

<br />


<p>本站文章如果没有特别说明，均为<strong>原创</strong>，转载请以<strong>链接</strong>方式注明本文地址：<a href="http://tinyxd.me/blog/2012/07/11/soft-interrupt-and-tasklet/">http://tinyxd.me/blog/2012/07/11/soft-interrupt-and-tasklet/</a></p>
]]></content>
  </entry>
  
</feed>

<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Tag: kernel | Keen on Art of Tech]]></title>
  <link href="http://tinyxd.me/tags/kernel/atom.xml" rel="self"/>
  <link href="http://tinyxd.me/"/>
  <updated>2012-07-27T12:47:29+08:00</updated>
  <id>http://tinyxd.me/</id>
  <author>
    <name><![CDATA[Tiny]]></name>
    <email><![CDATA[admin@tinyxd.me]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[linux进程调度]]></title>
    <link href="http://tinyxd.me/blog/2012/07/26/linux-process-scheduling/"/>
    <updated>2012-07-26T23:18:00+08:00</updated>
    <id>http://tinyxd.me/blog/2012/07/26/linux-process-scheduling</id>
    <content type="html"><![CDATA[<p><strong><em>摘要</em></strong>：linux与任何分时系统一样，通过一个进程到另一个进程的快速切换，达到表面上看来的多个进程同时执行的神奇效果。</p>

<br />


<p>Linux的调度基于分时（time sharing）技术：多个进程以“时间多路复用”方式运行，因为CPU的时间被分成“片（slice）”，给每个可运行进程分配一片。</p>

<p>在linux中，进程的优先级是动态的。调度程序跟踪进程正在做什么，并周期性地调整它们的优先级。</p>

<h2>分类 </h2>

<p>传统上，把进程分类为“IO受限（IO bound）”或“CPU受限（CPU bound）”。前者频繁使用IO设备，并花费很多时间等待IO操作的完成；而后者则需要大量CPU时间的数值计算应用程序。</p>

<p>另一种分类方法把进程区分为三类：交互式进程（interactive process）、批处理进程（batch process、实时进程（real-time process）。</p>

<!--more-->


<p></p>

<h2>进程的抢占 </h2>

<p>Linux的进程是抢占式的。根据优先级和时间片是否国企决定是否可以被抢占。</p>

<p>Linux2.6内核是抢占式的，这意味着进程无论是处于内核态还是用户态，都可能被抢占。</p>

<h2>调度算法 </h2>

<p>调度程序总能成功地找到要执行的进程。事实上，总是至少有一个可运行进程，即swapper进程，它的PID等于0，而且它只有在CPU不能执行其他进程时才执行。</p>

<p>每个Linux进程总是按照下面的调度类型被调度：</p>

<p>SCHED_FIFO（先进先出的实时进程）：直到先被执行的进程变为非可执行状态，后来的进程才被调度执行。在这种策略下，先来的进程可以执行sched_yield系统调用，自愿放弃CPU，以让权给后来的进程；</p>

<p>SCHED_RR（时间片轮转的实时进程）：轮转调度。内核为实时进程分配时间片，在时间片用完时，让下一个进程使用CPU；</p>

<p>SCHED_NORMAL（普通的分时进程）。</p>

<h2>普通进程的调度 (参考自<a href="http://hi.baidu.com/_kouu/blog/item/52471ab5e90e7c788ad4b24a.html">linux进程调度浅析</a>) </h2>

<p>实时进程调度的中心思想是，让处于可执行状态的最高优先级的实时进程尽可能地占有CPU，因为它有实时需求；而普通进程则被认为是没有实时需求的进程，于是调度程序力图让各个处于可执行状态的普通进程和平共处地分享CPU，从而让用户觉得这些进程是同时运行的。</p>

<p>每个普通进程都有它自己的静态优先级，还有动态优先级。</p>

<p>与实时进程相比，普通进程的调度要复杂得多。内核需要考虑两件麻烦事：</p>

<p>一、动态调整进程的优先级</p>

<p>按进程的行为特征，可以将进程分为“交互式进程”和“批处理进程”：</p>

<p>交互式进程（如桌面程序、服务器、等）主要的任务是与外界交互。这样的进程应该具有较高的优先级，它们总是睡眠等待外界的输入。而在输入到来，内核将其唤醒时，它们又应该很快被调度执行，以做出响应。比如一个桌面程序，如果鼠标点击后半秒种还没反应，用户就会感觉系统“卡”了；</p>

<p>批处理进程（如编译程序）主要的任务是做持续的运算，因而它们会持续处于可执行状态。这样的进程一般不需要高优先级，比如编译程序多运行了几秒种，用户多半不会太在意；</p>

<p>如果用户能够明确知道进程应该有怎样的优先级，可以通过nice、setpriority系统调用来对优先级进行设置。（如果要提高进程的优先级，要求用户进程具有CAP_SYS_NICE能力。）</p>

<p>然而应用程序未必就像桌面程序、编译程序这样典型。程序的行为可能五花八门，可能一会儿像交互式进程，一会儿又像批处理进程。以致于用户难以给它设置一个合适的优先级。</p>

<p>再者，即使用户明确知道一个进程是交互式还是批处理，也多半碍于权限或因为偷懒而不去设置进程的优先级。（你又是否为某个程序设置过优先级呢？）</p>

<p>于是，最终，区分交互式进程和批处理进程的重任就落到了内核的调度程序上。</p>

<p>调度程序关注进程近一段时间内的表现（主要是检查其睡眠时间和运行时间），根据一些经验性的公式，判断它现在是交互式的还是批处理的？程度如何？最后决定给它的优先级做一定的调整。</p>

<p>进程的优先级被动态调整后，就出现了两个优先级：</p>

<p>1、用户程序设置的优先级（如果未设置，则使用默认值），称为静态优先级。这是进程优先级的基准，在进程执行的过程中往往是不改变的；</p>

<p>2、优先级动态调整后，实际生效的优先级。这个值是可能时时刻刻都在变化的；</p>

<p>二、调度的公平性</p>

<p>在支持多进程的系统中，理想情况下，各个进程应该是根据其优先级公平地占有CPU。而不会出现“谁运气好谁占得多”这样的不可控的情况。</p>

<p>linux实现公平调度基本上是两种思路：</p>

<p>1、给处于可执行状态的进程分配时间片（按照优先级），用完时间片的进程被放到“过期队列”中。等可执行状态的进程都过期了，再重新分配时间片；</p>

<p>2、动态调整进程的优先级。随着进程在CPU上运行，其优先级被不断调低，以便其他优先级较低的进程得到运行机会；</p>

<p>后一种方式有更小的调度粒度，并且将“公平性”与“动态调整优先级”两件事情合而为一，大大简化了内核调度程序的代码。因此，这种方式也成为内核调度程序的新宠。</p>

<p>强调一下，以上两点都是仅针对普通进程的。而对于实时进程，内核既不能自作多情地去动态调整优先级，也没有什么公平性可言。</p>

<p>普通进程具体的调度算法非常复杂，并且随linux内核版本的演变也在不断更替（不仅仅是简单的调整），所以本文就不继续深入了。有兴趣的朋友可以参考下面的链接：</p>

<p><a href="http://www.ibm.com/developerworks/cn/linux/l-cn-scheduler/">《Linux 调度器发展简述》</a></p>

<p><a href="http://blog.chinaunix.net/u1/42957/showart.php?id=337597">《鼠眼看Linux调度器》</a></p>

<p><a href="http://blog.chinaunix.net/u1/42957/showart.php?id=337604">《鼠眼再看Linux调度器［1］》</a></p>

<p><a href="http://blog.chinaunix.net/u1/42957/showart.php?id=337607">《鼠眼再看Linux调度器［2］》</a></p>

<h2>实时进程的调度 </h2>

<p>每个实时进程都与一个实时优先级相关，实时优先级是一个范围从1（最高优先级）~99（最低优先级）的值。调度程序总是让优先级高的进程运行，换句话说，实时进程运行的过程中，禁止低优先级的进程的执行。与普通进程相反，实时进程总是被当成活动进程。用户可以通过系统调用sched_setparam()和sched_setscheduler()改变进程的实时优先级。</p>

<p>只有在下述时间之一发生时，实时进程才会被另一个进程取代：</p>

<pre><code>进程被另外一个具有更高实时优先级的实时进程抢占 

进程执行了阻塞操作并进入睡眠（处于TASK_INTERRUPTIBLE或TASK_UNINTERRUPTIBLE状态）。 

进程停止（处于TASK_STOPPED或TASK_TRACED状态）或被杀死（处于EXIT_ZOMBIE或EXIT_DEAD状态）。 

进程通过调用系统调用sched_yield()自愿放弃CPU。 

进程是基于时间片轮转的实时进程（SCHED_RR），而且用完了它的时间片。 
</code></pre>

<p>数据结构runqueue是Linux2.6调度程序最重要的数据结构。系统中的每个CPU都有它自己的运行队列，所有的runqueue结构存放在runqueues每CPU变量中。</p>

<h2>调度程序所使用的函数 </h2>

<p>scheduler_tick()：维持当前最新的time_slice计数器</p>

<p>try_to_wake_up()：唤醒睡眠进程</p>

<p>recalc_task_prio()：更新进程的动态优先级</p>

<p>schedule()：选择要被执行的新进程</p>

<p>load_balance()：维持多处理器系统中运行队列的平衡</p>

<h2>多处理器系统中运行队列的平衡 </h2>

<p>Linux一直坚持采用对称多处理模式，这意味着，与其他CPU相比，内核不对一个CPU有任何偏向，但是，多处理器机器具有很多不同的风格，而且调度程序的实现随硬件特征的不同而有所不同，我们将特别关注下面三种不同类型的多处理器机器：</p>

<p>（1）标准的多处理器体系结构</p>

<p>直到最近，这是多处理器机器最普通的体系结构。这些机器所共有的RAM芯片集被所有CPU共享。</p>

<p>（2）超线程</p>

<p>超线程芯片是一个立刻执行几个执行线程的微处理器；它包括几个内部寄存器的拷贝，并快速在它们之间切换。这种由Intel发明的技术，使得当前线程在访问内存的间隙，处理器可以使用它的机器周期去执行另外一个线程。一个超线程的物理CPU可以被Linux看作几个不同的逻辑CPU。</p>

<p>（3）NUMA</p>

<p>把CPU和RAM以本地“结点”为单位分组，（通常一个结点包括一个CPU和几个RAM芯片）。内存仲裁器（一个使系统中的CPU以串型方式访问RAM的专用电路）是典型的多处理器系统的瓶颈。在NUMA体系结构中，当CPU访问与它同在一个结点中的“本地”RAM芯片时，几乎没有竞争，因此访问通常是非常快的。另一方面，访问它所属结点外的“远程”RAM芯片就非常慢。</p>

<h2>与调度相关的系统调用 </h2>

<p>nice()、getpriority()和setpriority()系统调用、sched_getaffinity()和sched_setaffinity()调用</p>

<h2>与实时进程相关的系统调用 </h2>

<p>sched_getscheduler()和sched_setscheduler()系统调用、sched_getparam()和sched_setparam()系统调用、sched_yield()系统调用、sched_get_priority_min()和 sched_get_priority_max()系统调用、sched_rr_get_interval()系统调用</p>

<br />


<p>本文章参考自《深入理解linux内核》。 <br/>
本站文章如果没有特别说明，均为<strong>原创</strong>，转载请以<strong>链接</strong>方式注明本文地址：<a href="http://tinyxd.me/blog/2012/07/26/linux-process-scheduling/">http://tinyxd.me/blog/2012/07/26/linux-process-scheduling/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[linux系统定时测量]]></title>
    <link href="http://tinyxd.me/blog/2012/07/26/linux-timing-measurements/"/>
    <updated>2012-07-26T12:48:00+08:00</updated>
    <id>http://tinyxd.me/blog/2012/07/26/linux-timing-measurements</id>
    <content type="html"><![CDATA[<p>Linux内核必须完成的两种主要定时测量：</p>

<p>保存当前的时间和日期，以便通过time()、ftime()、gettimeofday()系统调用把它们返回给用户程序，也可以由内核本身把当前时间作为文件和网络包的时间戳。</p>

<p>维持定时器，这种机制能够告诉内核或用户程序某一时间间隔已经过去了。</p>

<p>一般会遇到的几种时钟和定时器电路：实时时钟（Real Time Clock RTC）、时间戳计数器（Time Stamp Counter TSC）、可编程间隔定时器（Programmable Interval Timer PIT）、CPU本地定时器（APIC中）、高精度事件定时器（HPET）、ACPI电源管理定时器。</p>

<p>Linux的计时体系结构是一组与时间流相关的内核数据结构和函数。实际上，基于80x86多处理器机器所具有的计时体系结构与单处理器机器所具有的稍有不同：</p>

<p>在三处理器系统上，所有的计时活动都是由全局定时器（可以是可编程间隔定时器也可以是高精度事件定时器）产生的中断触发的。</p>

<!--more-->


<p>在多处理器系统上，所有普通的活动（像软定时器处理）都是由全局定时器产生的中断触发的，而具体的CPU的活动（像监控当前运行进程的执行时间）是由本地APIC定时器产生的中断触发的。</p>

<p>jiffies</p>

<p>jiffies是一个计数器，用来记录自系统启动依赖产生的节拍总数。启动时，内核将该变量初始化为0，此后，每次时钟中断处理程序都会增加该变量的值。因为一秒内时钟中断的次数等于Hz，所以jiffies一秒内增加的值也就为Hz。系统运行时间以秒为单位计算，就等于jiffies/Hz。</p>

<p>xtime</p>

<p>xtime变量存放当前时间和日期；它是一个timespec类型的数据结构，该结构有两个字段：</p>

<p>1.tv_sec 存放自1970年1月1日（UTC）午夜以来经过的秒数。</p>

<p>2.tv_nsec存放自上一秒开始经过的纳秒数（它的值域范围在0-999999999之间）</p>

<p>在单处理器系统上，所有与定时有关的活动都是由IRQ线0上的可编程间隔定时器产生的中断触发的。</p>

<p>多处理器系统可以依赖两种不同的时钟中断源：可编程间隔定时器或高精度事件定时器产生的中断，以及CPU本地定时器产生的中断(监管内核代码并检测当前进程在特定CPU上已经运行了多长时间)。</p>

<p>内核在于定时相关的其他任务中必须周期性地收集若干数据用于：</p>

<pre><code>检查运行进程的CPU资源限制  

更新与本地CPU工作负载有关的统计数  

计算平均系统负载  

监管内核代码  
</code></pre>

<p>软定时器和延迟函数</p>

<p>定时器是一种软件功能，即允许在将来的某个时期，函数在给定的时间间隔用完时被调用。超时（time-out）表示与定时器相关的时间间隔已经用完的那个时刻。</p>

<p>Linux考虑两种类型的定时器，即动态定时器（dynamic timer）和间隔定时器（interval timer）。第一种类型由内核使用，而间隔定时器可以由进程在用户态创建。</p>

<p>延迟函数</p>

<p>当内核需要等待一个较短的时间间隔（比方说，不超过几毫秒）时，就不需要使用软定时器。</p>

<p>开发驱动，需要较短的时间间隔，在以上情况下，内核使用udelay()和ndelay()函数：前者接收一个微妙级的时间间隔作为它的参数，并在指定的延迟结束后返回；后者与前者类似，但是指定的延迟参数是纳秒级的。</p>

<p>本文章参考自《深入理解linux内核》。 <br/>
本站文章如果没有特别说明，均为<strong>原创</strong>，转载请以<strong>链接</strong>方式注明本文地址：<a href="http://tinyxd.me/blog/2012/07/26/linux-timing-measurements/">http://tinyxd.me/blog/2012/07/26/linux-timing-measurements/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[linux内核同步]]></title>
    <link href="http://tinyxd.me/blog/2012/07/18/linux-kernel-synchronization/"/>
    <updated>2012-07-18T15:57:00+08:00</updated>
    <id>http://tinyxd.me/blog/2012/07/18/linux-kernel-synchronization</id>
    <content type="html"><![CDATA[<h2>内核抢占</h2>

<p>抢占内核的主要特点是：一个在内核态运行的进程，可能在执行内核函数期间被另外一个进程取代。 <br/>
只有当内核正在执行异常处理程序（尤其是系统调用），而且内核抢占没有被显式地禁用时，才可能抢占内核。 <br/>
内核使用的各种同步技术：每CPU变量、原子操作、内存屏障、自旋锁、信号量、顺序锁、本地中断的禁止、本地软中断的禁止、读-拷贝-更新（RCU）。</p>

<h2>每CPU变量</h2>

<p>主要是数据结构的数组，系统的每个CPU对应数组的一个元素。 <br/>
一个CPU不应该访问与其他CPU对应的数组元素。另外，它可以随意读或修改它自己的元素而不用担心出现竞争条件，因为它是唯一有资格这么做的CPU。在单处理器和多处理器系统中，内核抢占都可能使每CPU变量产生竞争条件。总的原则是内核控制路径应该在禁用抢占的情况下访问每CPU变量。</p>

<!--more-->


<h2>原子操作</h2>

<p>若干汇编语言指令具有“读-修改-写”类型----也就是说，它们访问存储器单元两次，第一次读原值，第二次写新值。 <br/>
Linux内核提供了一个专门的atomic_t类型（一个原子计数器）和一些专门的函数和宏。在多处理器系统中，每条这样的指令都有一个lock字节（“锁定内存总线，直到这条指令执行完成”）的前缀。</p>

<h2>优化和内存屏障</h2>

<p>当使用优化的编译器时，编译器为了优化可能会重新安排汇编语言指令以便寄存器以最优的方式使用。 <br/>
内存屏障（memory barrier）原语确保，在原语之后的操作开始执行之前，原语之前的操作已经完成。</p>

<h2>自旋锁</h2>

<p>自旋锁（spin lock）是用来在多处理器环境中工作的一种特殊的锁。如果内核控制路径发现自旋锁“开着”，就获取锁并继续自己的执行。相反，如果内核控制路径发现锁由运行在另一个CPU上的内核控制路径“锁着”，就在周围“旋转”。反复执行一条紧凑的循环指令，直到锁被释放。   <br/>
一般来说，由自旋锁所保护的每个临界区都是禁止内核抢占的。在单处理器系统上，这种锁本身并不起锁的作用，自旋锁原语仅仅是禁止或启用内核抢占。  <br/>
在linux中，每个自旋锁都用spinlock_t结构表示，其中包含两个字段：</p>

<p>1.slock----表示自旋锁的状态。（1--未加锁  负数和0--加锁） <br/>
2.break_lock----表示进程正在忙等自旋锁（只在内核支持SMP和内核抢占的情况下使用该标志）</p>

<p>读写自旋锁的引入是为了增加内核的并发能力。只要没有内核控制路径堆数据结构进行修改，读写自旋锁就允许多个内核控制路径读同一数据结构。允许对数据结构并发度可以提高系统性能。 <br/>
顺序锁：与读写自旋锁非常相似，只是它为写者赋予了较高的优先级；事实上，即使在读者正在读的时候也允许写者继续运行。这种策略的好处是写者永远不会等待（除非另外一个写者正在写），缺点是有些时候读者不得不反复多次读相同的数据直到获得有效的副本。 <br/>
当读者进入临界区时，不必禁用内核抢占；另一方面，由于写者获取自旋锁，所以它进入临界区时自动禁用内核抢占。 <br/>
一般来说，在满足以下条件时才能使用顺序锁：</p>

<p>1.被保护的数据结构不包括被写者修改和被读者间接引用的指针（否则，写者可能在读者的眼鼻下就修改指针）。 <br/>
2.读者的临界区代码没有副作用（否则，多个读者的操作会与单独的读操作有不同的结果）。</p>

<h2>读-拷贝-更新（RCU）</h2>

<p>读-拷贝-更新（RCU）是为了保护在多数情况下被多个CPU读的数据结构而设计的另一种同步技术。RCU允许多个读者和写者并发执行，RCU不使用锁，就是说它不使用被所有CPU共享的锁或计数器。  <br/>
RCU同步的关键思想是：</p>

<p>1.RCU只保护被动态分配并通过指针引用的数据结构。 <br/>
2.在被RCU保护的临界区中，任何内核控制路径都不能睡眠。</p>

<p>使用RCU技术的真正困难在于：写者修改指针时不能立即释放数据结构的旧副本。实际上，写着开始修改时，正在访问数据结构的读者可能还在读旧副本。只有在CPU上的所有（潜在的）读者都执行完宏rcu_read_unlock()之后，才可以释放旧副本。 <br/>
RCU是Linux2.6中新加的功能，用在网络层和虚拟文件系统中。</p>

<h2>信号量</h2>

<p>Linux提供两种信号量：</p>

<p>1.内核信号量，由内核控制路径使用 <br/>
2.Systerm V IPC 信号量，由用户态进程使用</p>

<p>内核信号量：类似于自旋锁，因为当锁关闭着时，它不允许内核控制路径继续进行。只有可以睡眠的函数才能获取内核信号量；中断处理程序和可延迟函数都不能使用内核控制量。 <br/>
内核信号量是struct semaphore类型的对象。 <br/>
TASK_INTERRUPTIBLE是可以被信号和wake_up()唤醒的，当信号到来时，进程会被设置为可运行。 <br/>
而TASK_UNINTERRUPTIBLE只能被wake_up()唤醒。 <br/>
读/写信号量：类似于前面的“读写自旋锁”，有一点不同的是，在信号量再次变为打开之前，等待进程挂起而不是自旋。内核以严格的FIFO顺序处理等待读写信号量的所有进程。 <br/>
每个读写信号量都是有rw_semaphore结构描述的。 <br/>
补充原语（completion）：其和信号量之间的真正区别在于如何使用等待队列中包含的自旋锁。在补充原语中，自旋锁用来确保complete()和wait_for_completion()不会并发执行。在信号量中，自旋锁用于避免并发执行的down()函数弄乱信号量的数据结构。</p>

<br />


<p>本文章参考自《深入理解linux内核》。 <br/>
本站文章如果没有特别说明，均为<strong>原创</strong>，转载请以<strong>链接</strong>方式注明本文地址：<a href="http://tinyxd.me/blog/2012/07/18/linux-kernel-synchronization/">http://tinyxd.me/blog/2012/07/18/linux-kernel-synchronization/</a></p>
]]></content>
  </entry>
  
</feed>

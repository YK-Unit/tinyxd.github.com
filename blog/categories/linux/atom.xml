<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: linux | Keen on Art of Tech]]></title>
  <link href="http://tinyxd.me/blog/categories/linux/atom.xml" rel="self"/>
  <link href="http://tinyxd.me/"/>
  <updated>2012-07-13T19:10:02+08:00</updated>
  <id>http://tinyxd.me/</id>
  <author>
    <name><![CDATA[Tiny]]></name>
    <email><![CDATA[admin@tinyxd.me]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[debian硬盘安装笔记]]></title>
    <link href="http://tinyxd.me/blog/2012/07/13/debian-setup/"/>
    <updated>2012-07-13T11:29:00+08:00</updated>
    <id>http://tinyxd.me/blog/2012/07/13/debian-setup</id>
    <content type="html"><![CDATA[<p>以前安装的时候都有光盘，这次换的机器，光驱坏了，那么有没有可能硬盘安装debian么？答案当然是可以的。 <br/>
1.下载debian最新ISO映像。 <br/>
2.下载安装所需要的vmlinuz  initrd.gz  boot.img.gz的这三个文件. <br/>
   下载地址：<a href="http://debian.osuosl.org/debian/dists/Debian6.0.5/main/installer-i386/current/images/hd-media/">http://debian.osuosl.org/debian/dists/Debian6.0.5/main/installer-i386/current/images/hd-media/</a> <br/>
3.下载grub4 for DOS    <br/>
下载地址：<a href="http://download.gna.org/grub4dos/grub4dos-0.4.4-2009-06-20.zip">http://download.gna.org/grub4dos/grub4dos-0.4.4-2009-06-20.zip</a></p>

<h2>安装过程：</h2>

<p>1、先把WINXP安装到C盘，这里唯一要说的是如果在安装WINXP时，选择了把C盘格式化为NTFS格式，那么之后安装用的DEBIAN的ISO文件就不能放在C盘了，必须放在FAT32格式的分区的根目录下。切记哦！否则硬盘安装DEBIAN时后找不到ISO文件哦。</p>

<p>WINXP安装好以后，把下载的GRUB4 for DOS包解压缩到C盘根目录，并将目录名改为grub，（个人认为不改也应该可以，不过没试，你可以试下呵），并进入GRUB目录将grldr文件复制到C盘根目录下；把vmlinuz、initrd.gz、boot.img.gz三个文件也复制到C盘根目录下：（基本上这个时候C盘是NTFS格式还是FAT32格式都没有关系的）</p>

<!--more-->


<p>2、打开C盘根目录下的boot.ini文件，打开boot.ini后在文件的最后一行加上    C:\grldr=”GRUB FOR DOS”</p>

<p>之后保存并关闭boot.ini文件。</p>

<p>3、把前面下载的DEBIAN安装CD的ISO文件复制到C盘根目录下，（要保证C盘是FAT32格式的），否则就复制到其它FAT32分区的根目录下。</p>

<p>4、以上检查没有问题后重新启动电脑，这时应该会出现如下启动菜单：</p>

<p>5、选第二项“GRUB FOR DOS”后，等屏幕出现如下画面时，按键盘上的“C”进入命令行状态。</p>

<p>6、按“C”键后屏幕显示如下：</p>

<p>7、这个时候输入如下三行命令：</p>

<pre><code>kernel (hd0,0)/vmlinuz

initrd (hd0,0)/initrd.gz

boot
</code></pre>

<p>这里的（hd0,0）指的是C盘，hd是指硬盘啦，第一个0是指电脑里的第一块硬盘，第二个0是指该硬盘上的第一个主分区，如果是第二个主分区就是（hd0,1）啦，如果你有兴趣上网查一下相关的知识了，这里不说了。</p>

<p>8、接下来就是安装debian，先会自动寻找ISO映像，找到后就开始安装，不再赘述。</p>

<p>9、如果你的C盘是FAT32格式的，ISO文件也复制到C盘根目录，那么就不用担心了，安装程序会自动找到的。（ISO文件在其它FAT32分区根目录下也一样，我已试过呵）</p>

<p>10、我的分区是这样的：/boot 200M  、2G swap、15G / 、其余大概40G的/home</p>

<p>11、等系统装好后，需要设置中文字体，还需要安装一些必要软件等等。见我以前的文章，下面是链接：<a href="http://blog.chinaunix.net/uid-26053577-id-3011222.html">http://blog.chinaunix.net/uid-26053577-id-3011222.html</a></p>

<p>本文章来自本人<strong><a href="http://blog.chinaunix.net/uid/26053577.html">ChinaUnix</a></strong>博客。  <br/>
本站文章如果没有特别说明，均为<strong>原创</strong>，转载请以<strong>链接</strong>方式注明本文地址：<a href="http://tinyxd.me/blog/2012/07/13/debian-setup/">http://tinyxd.me/blog/2012/07/13/debian-setup/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[linux工作队列]]></title>
    <link href="http://tinyxd.me/blog/2012/07/11/linux-workqueue/"/>
    <updated>2012-07-11T23:08:00+08:00</updated>
    <id>http://tinyxd.me/blog/2012/07/11/linux-workqueue</id>
    <content type="html"><![CDATA[<h2>工作队列(workqueue)</h2>

<p>Linux中的Workqueue机制就是为了简化内核线程的创建。通过调用workqueue的接口就能创建内核线程。并且可以根据当前系统CPU的个数创建线程的数量，使得线程处理的事务能够并行化。</p>

<p>workqueue是内核中实现简单而有效的机制，他显然简化了内核daemon的创建，方便了用户的编程，</p>

<p>尽管可延迟函数和工作队列非常相似，但是它们的区别还是很大的。主要区别在于：可延迟函数运行在中断上下文中，而工作队列中的函数运行在进程上下文中。执行可阻塞函数（例如：需要访问磁盘数据块的函数）的唯一方式是在进程上下文中运行。因为在中断上下文中不可能发生进程切换。可延迟函数和工作队列中的函数都不能访问进程的用户态地址空间。事实上，可延迟函数被执行时不可能有任何正在运行的进程。另一方面，工作队列中的函数是由内核线程来执行的，因此，根本不存在它要访问的用户态地址空间。</p>

<h2>工作队列的数据结构</h2>

<p style="text-indent:2em">与工作队列相关的主要数据结构有两个：cpu_workqueue_struct和work_struct。名为workqueue_struct的描述符，包括一个有NR_CPUS个元素的数组，NR_CPUS是系统中CPU的最大数量（双核是有两个工作队列）。每个元素都是cpu_workqueue_struct类型的描述符。 

</p>


<p></p>

<!--more-->


<p>work_struct结构是对任务的抽象。在该结构中需要维护具体的任务方法，需要处理的数据，以及任务处理的时间。该结构定义如下：</p>

<p>``` c
struct work_struct {</p>

<pre><code>unsigned long pending;                /*如果函数已经在工作队列链表中，该字段值设为1，否则设为0*/ 
    struct list_head entry;                  /* 将任务挂载到queue的挂载点 */ 
    void (*func)(void *);                   /* 任务方法 */ 
    void *data;                                  /* 任务处理的数据*/ 
   void *wq_data;                           /* work的属主（通常是指向cpu_workqueue_struct描述符的父结点的指针） */ 
   strut timer_list timer;                   /* 任务延时处理定时器 */ 
</code></pre>

<p>};
```</p>

<h2>工作队列函数</h2>

<p>当用户调用workqueue的初始化接口create_workqueue或者create_singlethread_workqueue对workqueue队列进行初始化时，内核就开始为用户分配一个workqueue对象，并且将其链到一个全局的workqueue队列中。然后Linux根据当前CPU的情况，为workqueue对象分配与CPU个数相同的cpu_workqueue_struct对象，每个cpu_workqueue_struct对象都会存在一条任务队列。紧接着，Linux为每个cpu_workqueue_struct对象分配一个内核thread，即内核daemon去处理每个队列中的任务。至此，用户调用初始化接口将workqueue初始化完毕，返回workqueue的指针。</p>

<p>在初始化workqueue过程中，内核需要初始化内核线程，注册的内核线程工作比较简单，就是不断的扫描对应cpu_workqueue_struct中的任务队列，从中获取一个有效任务，然后执行该任务。所以如果任务队列为空，那么内核daemon就在cpu_workqueue_struct中的等待队列上睡眠，直到有人唤醒daemon去处理任务队列.</p>

<p>Workqueue初始化完毕之后，将任务运行的上下文环境构建起来了，但是具体还没有可执行的任务，所以，需要定义具体的work_struct对象。然后将work_struct加入到任务队列中，Linux会唤醒daemon去处理任务。</p>

<p>queue_work()（封装在work_struct描述符中）把函数插入工作队列，它接收wq和work两个指针。wq指向workqueue_struct描述符，work指向work_struct描述符。queue_work（）主要执行下面的步骤：</p>

<p>1.检查要插入的函数是否已经在工作队列中（work->pending字段等于1），如果是就结束。</p>

<p>2.把work_struct描述符加到工作队列链表中，然后把work->pending置为1。</p>

<p>3.如果工作者线程在本地CPU的cpu_workqueue_struct描述符的more_work等待队列上睡眠，该函数唤醒这个线程。</p>

<h2>预定义工作队列</h2>

<p>在绝大多数情况下，为了运行一个函数而创建整个工作者线程开销太大了。因此，内核引入叫做events的预定义工作队列，所有的内核开发者都可以随便使用它。预定义工作队列只是一个包含不同内核层函数和IO驱动程序的标准工作队列，它的workququq_struct描述符存放在keventd_wq数组中。</p>

<p>工作队列编程接口：</p>

<table border="1"> 
<tr> 
    <th>序号</th> 
    <th>接口函数</th>  
    <th>函数说明</th> 
</tr>  
<tr>  
    <td>1</td>  
    <td>create_workqueue</td> 
    <td>用于创建一个workqueue队列，为系统中的每个CPU都创建一个内核线程。输入参数：@name：workqueue的名称</td> 
</tr>  
<tr>  
    <td>2</td>  
    <td>create_singlethread_workqueue</td>  
    <td>用于创建一个workqueue队列，为系统中的每个CPU都创建一个内核线程。输入参数：@name：workqueue的名称</td> 
</tr> 
<tr> 
    <td>3</td> 
    <td>destroy_workqueue</td> 
    <td>释放workqueue队列。输入参数：@ workqueue_struct：需要释放的workqueue队列指针</td> 
</tr>  
<tr> 
    <td>4</td> 
    <td>schedule_work</td> 
    <td>调度执行一个具体的任务，执行的任务将会被挂入Linux系统提供的workqueue——keventd_wq输入参数：@ work_struct：具体任务对象指针 
</td> 
</tr>  
<tr> 
    <td>5</td> 
    <td>schedule_delayed_work</td> 
    <td>延迟一定时间去执行一个具体的任务，功能与schedule_work类似，多了一个延迟时间，输入参数：@work_struct：具体任务对象指针@delay：延迟时间</td> 
</tr>  
<tr> 
    <td>6</td> 
    <td>queue_work</td> 
    <td>调度执行一个指定workqueue中的任务。输入参数：@ workqueue_struct：指定的workqueue指针@work_struct：具体任务对象指针</td> 
</tr>  
<tr> 
    <td>7</td> 
    <td>queue_delayed_work</td> 
    <td>延迟调度执行一个指定workqueue中的任务，功能与queue_work类似，输入参数多了一个delay</td> 
</tr>  
</table>


<p></p>

<p>预定义工作队列支持函数:</p>

<table > 
<tr> 
    <td>预定义工作队列函数</td> 
    <td>等价的标准工作队列函数</td> 
</tr> 
<tr> 
    <td>schedule_work(w)</td>     
    <td>queue_work( keventd_wq,w)</td> 
</tr> 
<tr> 
    <td>schedule_delayed_work(w,d)</td>     
    <td>queue_delayed_work(keventd_wq,w,d)在任何CPU上</td> 
</tr> 
<tr> 
    <td>schedule_delayed_work_on(cpu,w,d)</td>     
    <td>queue_delayed_work(keventd_wq,w,d)（在某个指定的CPU上）</td> 
</tr> 
<tr> 
    <td>flush_scheduled_work</td>     
    <td>flush_workqueue(keventd_wq)</td> 
</tr> 
</table>


<p></p>

<p>除了一般的events队列，在linux2.6中你还会发现一些专用的工作队列。其中最重要的是快设备层使用的kblockd工作队列。</p>

<p>本文章整理自网络和《深入理解linux内核》。  <br/>
本站文章如果没有特别说明，均为<strong>原创</strong>，转载请以<strong>链接</strong>方式注明本文地址：<a href="http://tinyxd.me/blog/2012/07/11/linux-workqueue/">http://tinyxd.me/blog/2012/07/11/linux-workqueue/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[软中断和tasklet]]></title>
    <link href="http://tinyxd.me/blog/2012/07/11/soft-interrupt-and-tasklet/"/>
    <updated>2012-07-11T22:57:00+08:00</updated>
    <id>http://tinyxd.me/blog/2012/07/11/soft-interrupt-and-tasklet</id>
    <content type="html"><![CDATA[<h2>软中断：</h2>

<p>Linux中的软中断机制用于系统中对时间要求最严格以及最重要的中断下半部进行使用。在系统设计过程中，大家都清楚中断上下文不能处理太多的事情，需要快速的返回，否则很容易导致中断事件的丢失，所以这就产生了一个问题：中断发生之后的事务处理由谁来完成？在前后台程序中，由于只有中断上下文和一个任务上下文，所以中断上下文触发事件，设置标记位，任务上下文循环扫描标记位，执行相应的动作，也就是中断发生之后的事情由任务来完成了，只不过任务上下文采用扫描的方式，实时性不能得到保证。在Linux系统和Windows系统中，这个不断循环的任务就是本文所要讲述的软中断daemon。在Windows中处理耗时的中断事务称之为中断延迟处理，在Linux中称之为中断下半部，显然中断上半部处理清中断之类十分清闲的动作，然后在退出中断服务程序时触发中断下半部，完成具体的功能。</p>

<p>在Linux中，中断下半部的实现基于软中断机制。所以理清楚软中断机制的原理，那么中断下半部的实现也就非常简单了。通过上述的描述，大家也应该清楚为什么要定义软中断机制了，一句话就是为了要处理对时间要求苛刻的任务，恰好中断下半部就有这样的需求，所以其实现采用了软中断机制。</p>

<p>linux2.6中使用的软中断：</p>

<p>``` c interrupt.h
enum
{</p>

<pre><code>HI_SOFTIRQ=0, /*用于高优先级的tasklet(下标(优先级0))*/
TIMER_SOFTIRQ, /*用于定时器的下半部(下标(优先级1))*/
NET_TX_SOFTIRQ,/*用于网络层发包(下标(优先级2))*/
NET_RX_SOFTIRQ, /*用于网络层收报(下标(优先级3))*/
SCSI_SOFTIRQ, /*用于scsi设备(下标(优先级4))*/
TASKLET_SOFTIRQ /*用于低优先级的tasklet(下标(优先级5))*/
</code></pre>

<p>};
```</p>

<!--more-->


<p>一个软中断的下标决定了它的优先级；低下标意味着高优先级，因为软中断函数将从下标0开始执行。</p>

<h2>tasklet:</h2>

<p>Tasklet为一个软中断，考虑到优先级问题，分别占用了向量表中的0号和5号软中断。</p>

<p>tasklet是IO驱动程序中实现可延迟函数的首选方法。其建立在两个叫HI_SOFTIRQ和TASKLET_SOFTIRQ的软中断之上。</p>

<h2>软中断和tasklet</h2>

<p>软中断和tasklet有密切的关系，tasklet是在软中断之上实现。事实上，出现在内核代码中的术语“软中断（softirq）” 常常表示可延迟函数的所有种类。另外一种被广泛使用的术语是“中断上下文”：表示内核当前正在执行一个中断处理程序或一个可延迟的函数。</p>

<p>软中断的分配是静态的（即在编译时定义），而tasklet的分配和初始化可以在运行时进行（例如：安装一个内核模块时）。软中断（即便是同一种类型的软中断）可以并发地运行在多个CPU上。因此，软中断是可重入函数而且必须明确地使用自旋锁保护其数据结构。tasklet不必担心这些问题，因为内核对tasklet的执行进行了更加严格的控制。相同类型的tasklet总是被串行地执行，换句话说就是：不能在两个CPU上同时运行相同类型的tasklet。但是，类型不同的tasklet可以在几个CPU上并发执行。tasklet的串行化使tasklet函数不必是可重入的，因此简化了设备驱动程序开发者的工作。</p>

<p>一般而言，可延迟函数上可以执行四种函数：初始化、激活、屏蔽和执行。</p>

<h2>软中断的主要数据结构</h2>

<p>软中断的主要数据结构是softirq_vec数组，该数组包含类型为softirq_action的32个元素。一个软中断的优先级是相应的softirq_action元素在数组内的下标，只有前六个被有效使用。</p>

<p>/*表示softirq最多可以有32种类型，实际上linux只使用了六种，见文件interrupt.h*/</p>

<p>static struct softirq_action softirq_vec[32] __cacheline_aligned_in_smp;</p>

<p>softirq_action数据结构包括两个字段：指向软中断函数的一个action指针和指向软中断函数需要的通用数据结构的data指针。</p>

<p>还有一个关键的字段是32位的preempt_cout字段，用它来跟踪内核抢占和内核控制路径的嵌套，该字段存放在每个进程描述符的thread_info字段中。</p>

<p>对于softirq，linux kernel中是在中断处理程序执行的，具体的路径为：</p>

<pre><code>do_IRQ() --&gt; irq_exit() --&gt; invoke_softirq() --&gt; do_softirq() --&gt; __do_softirq() 
</code></pre>

<p>在__do_softirq()中有这么一段代码：  <br/>
``` c</p>

<pre><code>    do { 

            if (pending &amp; 1) { 

                    h-&gt;action(h); 

                    rcu_bh_qsctr_inc(cpu); 

            } 

            h++; 

            pending &gt;&gt;= 1; 

    } while (pending); 
</code></pre>

<p>```</p>

<p>你看，这里就是对softirq进行处理了，因为pengding是一个__u32的类型，所以每一位都对应了一种softirq，正好是32种（linux kernel中实际上只使用了前6种 ）. h->action(h),就是运行softirq的处理函数。</p>

<p>对于tasklet，前面已经说了，是一种特殊的softirq，具体就是第0和第5种softirq，所以说tasklet是基于softirq来实现的。</p>

<p>tasklet既然对应第0和第5种softirq，那么就应该有对应的处理函数，以便h->action()会运行tasklet的处理函数。</p>

<p>我们看代码：
``` c softirq.c</p>

<p>void __init softirq_init(void)</p>

<p>{</p>

<pre><code>    open_softirq(TASKLET_SOFTIRQ, tasklet_action, NULL); 

    open_softirq(HI_SOFTIRQ, tasklet_hi_action, NULL); 
</code></pre>

<p>}
```</p>

<p>这里注册了两种tasklet所在的softirq的处理函数，分别对应高优先级的tasklet和低优先级的tasklet。</p>

<p>我们看低优先级的吧（高优先级的也一样）。
``` c tasklet_action
static void tasklet_action(struct softirq_action *a)</p>

<p>{</p>

<pre><code>    struct tasklet_struct *list; 

    local_irq_disable(); 

    list = __get_cpu_var(tasklet_vec).list; 

    __get_cpu_var(tasklet_vec).list = NULL; 

    local_irq_enable(); 

    while (list) { 

            struct tasklet_struct *t = list; 

            list = list-&gt;next; 

            if (tasklet_trylock(t)) { 

                    if (!atomic_read(&amp;t-&gt;count)) { 

                            if (!test_and_clear_bit(TASKLET_STATE_SCHED, &amp;t-&gt;state)) 

                                    BUG(); 

                            t-&gt;func(t-&gt;data); 

                            tasklet_unlock(t); 

                            continue; 

                    } 

                    tasklet_unlock(t); 

            } 

            local_irq_disable(); 

            t-&gt;next = __get_cpu_var(tasklet_vec).list; 

            __get_cpu_var(tasklet_vec).list = t; 

            __raise_softirq_irqoff(TASKLET_SOFTIRQ);   

            local_irq_enable(); 

    } 
</code></pre>

<p>}
<code>
你看，在运行softirq的处理时（__do_softirq），对于
</code> c</p>

<pre><code>    do { 

            if (pending &amp; 1) { 

                    h-&gt;action(h); 

                    rcu_bh_qsctr_inc(cpu); 

            } 

            h++; 

            pending &gt;&gt;= 1; 

    } while (pending); 
</code></pre>

<p>```
如果tasklet有任务需要处理，会运行到h->action()，这个函数指针就会指向tasklet_action()，然后在tasklet_action()里再去执行tasklet对应的各个任务，这些任务都是挂在一个全局链表里面的，具体的代码这里就不分析了。</p>

<p>另外， softirq在smp中是可能被同时运行的，所以softirq的处理函数必须被编写成可重入的函数。</p>

<p>但tasklet是不会在多个cpu之中同时运行的，所以tasklet的处理函数可以编写成不可重入的函数，这样就减轻了编程人员的负担。</p>

<h2>ksoftirqd内核线程</h2>

<p>在最近的内核版本中，每个CPU都有自己的ksoftirqd/n内核线程（这里，n为CPU的逻辑号）。每个ksoftirqd/n内核线程都运行ksoftirqd()函数。在预期的时间内处理挂起的软中断。</p>

<br />


<p>本站文章如果没有特别说明，均为<strong>原创</strong>，转载请以<strong>链接</strong>方式注明本文地址：<a href="http://tinyxd.me/blog/2012/07/11/soft-interrupt-and-tasklet/">http://tinyxd.me/blog/2012/07/11/soft-interrupt-and-tasklet/</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[linux文件系统]]></title>
    <link href="http://tinyxd.me/blog/2012/07/10/linux-filesysterm/"/>
    <updated>2012-07-10T23:26:00+08:00</updated>
    <id>http://tinyxd.me/blog/2012/07/10/linux-filesysterm</id>
    <content type="html"><![CDATA[<p>linux文件操作：文件操作系统调用（create、open、read、write、lseek、close）、c库文件操作（fopen、fscanf、fprintf、fread、fwrite、fseek、fclose等）。</p>

<p>linux文件系统：在设备驱动程序的设计中，一般会关心file和inode这两个结构体。</p>

<h2>devfs、udev、sysfs的区别：</h2>

<p>以下内容来自<a href="http://blog.csdn.net/yeqishi/article/details/5491413">devfs, sysfs, udev文件系统区别</a> 和 linux设备驱动开发详解（第二版）</p>

<h3>一.devfs设备文件简略介绍(2.6版本以后内核都采用udev设备文件系统)</h3>

<p>devfs(设备文件系统)是由2.4内核引入的，具有如下优点：</p>

<p>1.可以通过程序在设备初始化时在/dev目录下创建设备文件，卸载时将它删除。</p>

<p>2.设备驱动程序可以指定设备号,所有者,和权限位,用户空间可以修改所有者和权限位。</p>

<p>3.不再需要为设备驱动程序分配主设备号以及处理的次设备号，在程序可以直接给register_chrdev()传递0主设备号以动态获得可用的主设备号，并在devfs_register() 中指定次设备号。</p>

<!--more-->


<p>在2.6内核以前一直使用的是 devfs，devfs挂载于/dev目录下，提供了一种类似于文件的方法来管理位于/dev目录下的所有设备，我们知道 /dev目录下的每一个文件都对应的是一个设备，至于当前该设备存在与否先且不论，而且这些特殊文件是位于根文件系统上的，在制作文件系统 的时候我们就已经建立了这些设备文件，因此通过操作这些特殊文件，可以实现与内核进行交互。但是devfs文件系统有一些缺点，例如：不确定的设备映射，有时一个设备映射的设备文件可能不同，例如我的U盘可能对应sda有可能对应sdb；没有足够的主/辅设备号，当设备过多的时候，显然这会成为一个问题；/dev目录下文件太多而且不能表示当前系统上的实际设备；命名不够灵活，不能任意指定等等。</p>

<br />


<h3>二.sysfs</h3>

<p>从Fedora 2开始，在根目录下会有一个/sys目录，mount 看一下，这个目录挂装了一个sysfs的文件系统。</p>

<p>Linux 2.6 的内核引入了 sysfs 文件系统。sysfs 被看成是与 proc，devfs，和 devpty 同类别的文件系统。sysfs 把连接在系统上的设备和总线组织成为一个分级的文件，它们可以被从用户的空间存取到。这是被设计用来处理那些以前驻留在 /proc/ 的设备和驱动程序指定的选件以及用来处理那些以前由 devfs 提供支持的动态加载设备。在早期的 sysfs 实现中，一些驱动和应用仍然被当做老的 proc 条目。但是 sysfs 是未来的发展方向。</p>

<p>sysfs 被加载在 /sys/ 系统中。它所包括的目录可以使用不同的方式来治理连接在系统上的设备。/sysfs/ 中的子目录包括：</p>

<p>/devices/ 目录这个目录包括 /css0/。它的子目录代表了所有被 Linux 内核检测到的子通道。子通道目录的命名格式是 0.0.nnnn，其中的 nnnn 是子通道的十六进制代码 (0到ffff)。子通道目录包括状态文件和其他代表实际设备的设备子目录。设备目录的格式是：0.0.xxxx，其中的 xxxx 是这个设备的单元地址。/devices/ 还包括了状态信息和设备的配置选项。</p>

<p>/bus/ 目录这个目录包括了 /ccw/ 和 /ccwgroup/ 两个子目录。CCW 设备可以通过使用通道命令来存取。在 /ccw/ 目录中的设备只使用一个子通道。CCW 组设备也可以通过使用通道命令来存取，但是它们的每个设备使用多于一个的子通道。比如：一个3390-3 DASD 设备使用一个子通道，但是一个 OSA 适配器的 QDIO 网络连接使用三个子通道。/ccw/ 和 /ccwgroup/ 目录都包括设备目录和驱动器目录：</p>

<p>/devices/ 目录包括了到 /sys/devices/css0/ 目录的设备目录的符号链接。/drivers 目录包括了所有由代表当前被系统加载的设备的驱动程序的目录。zFCP 驱动程序有一个目录在这里。/driver/ 目录包括了设备驱动程序的设置和它使用的符号链接 (/sys/devices/css0/ 目录)/class/ 目录/class/ 目录包括了代表由相似功能的设备组成的组 (ttys，SCSI 磁带驱动器，网络设备...)的目录。</p>

<p>/block/ 目录这个目录包括了系统中的每一个块设备的目录。块设备主要是磁盘类的设备，例如 DASD，回送设备，以及软件磁盘冗余阵列设备。一个与老版本 Linux 不同的是，使用 sysfs 系统的 Linux 需要使用设备在 sysfs 中的名字来指定设备。在一个 2.4 版本的内核映像中，zFCP 驱动程序是由它的设备地址来指定的。但是在 2.6 版本的内核映像中，它的驱动程序是由 0.0.1600 来指定的。</p>

<br />


<h3>三.udev设备文件详细介绍</h3>

<p>   devfs 存在的主要的问题是它处理设备检测、创建和命名的方式，其中设备节点的命名可能是最严重的问题。一般可接受的方式是，如果设备名是可配置的，那么设备命名策略应该由系统管理员决定，而不是由某些开发者强制规定。devfs 文件系统还存在竞争条件(race conditions)的问题，这是它天生的设计缺陷，不对内核做彻底的修改就无法修正这个问题。</p>

<p>   上篇文章简单介绍sysfs文件系统，您可能想知道 sysfs 是怎么认出系统中存在的设备以及应该使用什么设备号。对于已经编入内核的驱动程序，当被内核检测到的时候，会直接在 sysfs 中注册其对象；对于编译成模块的驱动程序，当模块载入的时候才会这样做。一旦挂载了 sysfs 文件系统(挂载到 /sys)，内建的驱动程序在 sysfs 注册的数据就可以被用户空间的进程使用，并提供给 udev 以创建设备节点。</p>

<p>   udev 初始化脚本负责在 Linux 启动的时候创建设备节点，该脚本首先将 /sbin/udevsend 注册为热插拔事件处理程序。热插拔事件(随后将讨论)本不应该在这个阶段发生，注册 udev 只是为了以防万一。然后 udevstart 遍历 /sys 文件系统，并在 /dev 目录下创建符合描述的设备。例如，/sys/class/tty/vcs/dev 里含有"7:0"字符串，udevstart 就根据这个字符串创建主设备号为 7 、次设备号为0 的 /dev/vcs 设备。udevstart 创建的每个设备的名字和权限由 /etc/udev/rules.d/ 目录下的文件指定的规则来设置。如果 udev 找不到所创建设备的权限文件，就将其权限设置为缺省的 660 ，所有者为 root:root 。</p>

<p>   上面的步骤完成后，那些已经存在并且已经内建驱动的设备就可以使用了，那么以模块驱动的设备呢？</p>

<p>   前面我们提到了"热插拔事件处理程序"的概念，当内核检测到一个新设备连接时，内核会产生一个热插拔事件，</p>

<p>   并在 /proc/sys/kernel/hotplug 文件里查找处理设备连接的用户空间程序。udev 初始化脚本将 udevsend 注册为该处理程序。</p>

<p>   当产生热插拔事件的时候，内核让 udev 在 /sys 文件系统里检测与新设备的有关信息，并为新设备在 /dev 里创建项目。</p>

<p>   大多数 Linux 发行版通过 /etc/modules.conf 配置文件来处理模块加载，对某个设备节点的访问导致相应的内核模块被加载。对 udev 这个方法就行不通了，因为在模块加载前，设备节点根本不存在。为了解决这个问题，在 LFS-Bootscripts 软件包里加入了 modules 启动脚本，以及 /etc/sysconfig/modules 文件。通过在 modules 文件里添加模块名，就可以在系统启动的时候加载这些模块，这样 udev 就可以检测到设备，并创建相应的设备节点了。如果插入的设备有一个驱动程序模块但是尚未加载，Hotplug 软件包就有用了，它就会响应上述的内核总线驱动热插拔事件并加载相应的模块，为其创建设备节点，这样设备就可以使用了。</p>

<p>   udev是一种工具，它能够根据系统中的硬件设备的状况动态更新设备文件，包括设备文件的创建，删除等。设备文件通常放在/dev目录下，使用udev 后,在/dev下面只包含系统中真实存在的设备。它于硬件平台无关的，位于用户空间，需要内核sysfs和tmpfs的支持，sysfs为udev提供设备入口和uevent通道，tmpfs为udev设备文件提供存放空间。</p>

<p>   显而易见<code>udev设备文件的优点</code>：</p>

<p>   1.udev完全在用户态工作，利用设备加入或移除时内核所发送的热插拔事件。在热插拔时，设备的详细信息会由内核输出到位于/sys的sysfs文件系统。udev的设备命名策略权限控制都在用户态完成的，它利用sysfs信息来进行创建设备文件节点。</p>

<p>   2.udev根据系统中的硬件设备的状态动态更新设备文件，进行设备文件的创建和删除等。</p>

<p>   注：使用udev,/dev目录下就会只包含系统中真正存在的设备。</p>

<p>   注：所有在 sysfs 中显示的设备都可以由 udev 来创建节点。如果内核中增加了其它设备的支持，</p>

<p>   udev 也就自动地可以为它们工作了。在init初始化之前，udev 可以被放入 initramfs 之中，并在每个设备被发现的时候运行。</p>

<p>   也可以让udev 工作在一个真的根分区被加载之后根据 /sys 的内容创建的初始 /dev 目录之中</p>

<br />


<h3>四.udev和devfs设备文件的对比</h3>

<p>1.udev能够实现所有devfs实现的功能。但udev运行在用户模式中，而devfs运行在内核中。</p>

<p>2.当一个并不存在的 /dev 节点被打开的时候， devfs一样自动加载驱动程序而udev确不能。</p>

<p>  udev设计时，是在设备被发现的时候加载模块，而不是当它被访问的时候。</p>

<p>  devfs这个功能对于一个配置正确的计算机是多余的。系统中所有的设备都应该产生</p>

<p>  hotplug 事件、加载恰当的驱动，而 udev 将会注意到这点并且为它创建对应的</p>

<p>  设备节点。如果你不想让所有的设备驱动停留在内存之中，应该使用其它东西来</p>

<p>  管理你的模块 (如脚本, modules.conf, 等等) 。</p>

<p>  其中devfs 用的方法导致了大量无用的 modprobe 尝试，以此程序探测设备是否存在。</p>

<p>  每个试探性探测都新建一个运行 modprobe 的进程，而几乎所有这些都是无用的。</p>

<p>3.udev是通过对内核产生的设备名增加别名的方式来达到上述目的的。前面说过，udev是用户模式程序，不会更改内核的行为。</p>

<p>因此，内核依然会我行我素地产生设备名如sda,sdb等。但是，udev可以根据设备的其他信息如总线（bus），生产商（vendor）等</p>

<p>不同来区分不同的设备，并产生设备文件。udev只要为这个设备文件取一个固定的文件名就可以解决这个问题。在后续对设备的操作中，</p>

<p>只要引用新的设备名就可以了。但为了保证最大限度的兼容，一般来说，</p>

<p>新设备名总是作为一个对内核自动产生的设备名的符号链接（link）来使用的。</p>

<p><strong>例如</strong>：内核产生了sda设备名，而根据信息，这个设备对应于是我的内置硬盘，那我就可以制定udev规则，让udev除了产生/dev/sda设备文件外，另外创建一个符号链接叫/dev/internalHD。这样，我在fstab文件中，就可以用/dev/internalHD来代替原来的 /dev/sda了。下次，由于某些原因，这个硬盘在内核中变成了sdb设备名了，那也不用着急，udev还会自动产生/dev/internalHD这个链接，并指向正确的/dev/sdb设备。所有其他的文件像fstab等都不用修改。</p>

<p>而在在2.6内核以前一直使用的是 devfs，devfs挂载于/dev目录下，提供了一种类似于文件的方法来管理位于/dev目录下的所有设备，但是devfs文件系统有一些缺点，例如：不确定的设备映射，有时一个设备映射的设备文件可能不同，例如我的U盘可能对应sda有可能对应sdb 。</p>

<p>注：可以用命令查看其中的信息，    udevinfo -a -p /sys/block/sda</p>

<p>在此之前的设备文件管理方法（静态文件和devfs）有几个缺点：</p>

<p><strong> 不确定的设备映射。</strong>特别是那些动态设备，比如USB设备，设备文件到实际设备的映射并不可靠和确定。举一个例子：如果你有两个USB打印机。一个可能称为 /dev/usb/lp0,另外一个便是/dev/usb/lp1。但是到底哪个是哪个并不清楚，lp0,lp1和实际的设备没有一一对应的关系，因为他可能因为发现设备的顺序，打印机本身关闭等原因而导致这种映射并不确定。理想的方式应该是：两个打印机应该采用基于他们的序列号或者其他标识信息的唯一设备文件来映射。但是静态文件和devfs都无法做到这点。</p>

<p><strong>没有足够的主/辅设备号。</strong>我们知道，每一个设备文件是有两个8位的数字：一个是主设备号 ，另外一个是辅设备号来分配的。这两个8位的数字加上设备类型（块设备或者字符设备）来唯一标识一个设备。不幸的是，关联这些身边的的数字并不足够。</p>

<p><strong>/dev目录下文件太多。</strong>一个系统采用静态设备文件关联的方式，那么这个目录下的文件必然是足够多。而同时你又不知道在你的系统上到底有那些设备文件是激活的。</p>

<p><strong>命名不够灵活。</strong>尽管devfs解决了以前的一些问题，但是它自身又带来了一些问题。其中一个就是命名不够灵活；你别想非常简单的就能修改设备文件的名字。缺省的devfs命令机制本身也很奇怪，他需要修改大量的配置文件和程序。;</p>

<p><strong>内核内存使用</strong>，devfs特有的另外一个问题是，作为内核驱动模块，devfs需要消耗大量的内存，特别当系统上有大量的设备时（比如上面我们提到的系统一个上有好几千磁盘时）</p>

<p>udev的目标是想解决上面提到的这些问题，他通采用用户空间(user-space)工具来管理/dev/目录树，他和文件系统分开。知道如何改变缺省配置能让你如何定制自己的系统，比如创建设备字符连接，改变设备文件属组，权限等。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[linux sleep 的用法]]></title>
    <link href="http://tinyxd.me/blog/2012/07/06/linux-sleep-usage/"/>
    <updated>2012-07-06T12:40:00+08:00</updated>
    <id>http://tinyxd.me/blog/2012/07/06/linux-sleep-usage</id>
    <content type="html"><![CDATA[<p><strong>应用程序</strong>：</p>

<pre><code>#include &lt;syswait.h&gt;
usleep(n) //n微秒
Sleep（n）//n毫秒
sleep（n）//n秒
</code></pre>

<!--more-->


<p><strong>驱动程序</strong>：</p>

<pre><code>#include &lt;linux/delay.h&gt;
mdelay(n) //milliseconds 其实现
</code></pre>

<p>```</p>

<h1>ifdef notdef</h1>

<h1>define mdelay(n) (\</h1>

<p>{unsigned long msec=(n); while (msec--) udelay(1000);})</p>

<h1>else</h1>

<h1>define mdelay(n) (\</h1>

<p>(__builtin_constant_p(n) &amp;&amp; (n)&lt;=MAX_UDELAY_MS) ? udelay((n)*1000) : \
({unsigned long msec=(n); while (msec--) udelay(1000);}))</p>

<h1>endif</h1>

<p>```
调用asm/delay.h的udelay,udelay应该是纳秒级的延时</p>

<p>dos:</p>

<pre><code>sleep(1); //停留1秒 
delay(100); //停留100毫秒   
</code></pre>

<p>Windows:</p>

<pre><code>Sleep(100); //停留100毫秒 
</code></pre>

<p>Linux:</p>

<pre><code>sleep(1); //停留1秒 
usleep(1000); //停留1毫秒 
</code></pre>

<p>每一个平台不太一样,最好自己定义一套跨平台的宏进行控制   <br/>
附：Linux下（使用的gcc的库），sleep()函数是以秒为单位的，sleep(1);就是休眠1秒。而MFC下的sleep()函数是以微秒为单位的，sleep(1000);才是休眠1秒。而如果在Linux下也用微妙为单位休眠，可以使用线程休眠函数:void usleep(unsigned long usec);当然，使用的时候别忘记#include &lt;system.h>哦。另外，linux下还有个delay()函数，原型为extern void delay(unsigned int msec);它可以延时msec*4毫秒，也就是如果想延时一秒钟的话，可以这么用 delay(250)。 <br/>
转载请注明出处：<a href="http://tinyxd.me/blog/2012/07/06/linux-sleep-usage/">http://tinyxd.me/blog/2012/07/06/linux-sleep-usage/</a></p>
]]></content>
  </entry>
  
</feed>
